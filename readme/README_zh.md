# 使用 TensorRT 部署深度学习语音增强模型

安装 TensorRT 见：[TensorRT 安装教程](./TRTSETUP_zh.md)

语音增强模型的部署可以分为 **离线推理** 和 **在线推理** 两类。离线推理是在预先准备的数据上进行模型推理，通常是一批样本或较长的语音信号。离线推理无实时要求，可使用高效的推理方法和资源分配策略。

在线推理是在实时场景中对实时生成的语音数据进行模型推理，如通过麦克风捕捉的连续语音信号。在线推理要求低延迟、高吞吐量以满足实时性的要求。

## 离线模型的部署
### 1. 转换为 ONNX 模型
对于离线模型，ONNX 的导出非常简单。唯一需要注意的是输入形状的时间维度的设置。尽管 `torch.onnx.export` 支持动态维度，但考虑到实际应用场景中该需求不大，我们选择固定时间维度为 563，对应 9 s 长度的音频数据。当离线处理时，若音频不足 9 s，对其进行补 0 处理；若音频大于 9 s，对其以 9 s 分段按批次处理。

`offline\dpcrn_onnx.py` 提供了 ONNX 模型的导出及推理，并评估了 ONNX 模型在 ONNXRuntime 上的推理速度。

### 2. 转换为 Engine 模型
我们使用 TensorRT 官方提供的转换工具 `trtexec.exe` 来进行模型从 ONNX 到 TensorRT 支持的 Engine 格式的转换，该工具位于 TensorRT 安装包内的 `bin` 目录下，使用方法为：
```
trtexec.exe --onnx=[onnx_path] ---saveEngine=[save_path]
```

`offline\dpcrn_trt.py` 提供了 Engine 模型的导出及推理，并评估了 Engine 模型的推理速度，结果如下表。

| **模型格式** | **推理框架** | **推理平台** | **平均推理速度 （ms）** | **最大推理速度 （ms）** | **最小推理速度 （ms）** |
|:-----------:|:-----------:|:-----------:|:----------------------:|:----------------------:|:----------------------:|
| ONNX | ONNXRuntime | CPU | 8.6 | 21.0 | 7.6|
| Engine |TensorRT| CUDA | 2.2 | 5.1 | 1.9 |

其中推理使用的 CPU 为 13th Gen Intel(R) Core(TM) i9-13900HX @ 2.20 GHz，使用的 CUDA 为 NVIDIA GeForce RTX 4080 Laptop GPU。推理过程重复进行了 1000 次，据此得到平均和最大最小推理速度。这里推理速度的定义为：音频处理时长 / 音频总时长。可以看到 TensorRT 框架的推理速度比 ONNXRuntime 提升了将近 4 倍。

## 在线模型的部署
在语音增强中，在线推理的应用场景更广泛且对模型的实时性要求更高，相应地，在线推理的部署也更复杂。在这里我们采用 **流式推理** 的方法，对实时数据流进行逐帧推理。在实现流式推理时，需要适当的数据缓冲机制、数据流管理和模型推理的流水线设计，以确保数据的连续性和推理的稳定性。

### 1. 转换为流式模型
RNN 天然适应流式推理，无需额外转换，相比之下，卷积层是神经网络中需要进行流式转换的主要部分。在 `online\modules` 中，我们在 `convolution.py` 中定义了流式卷积和流式转职卷积两种算子的实现，并且在 `convert.py` 中提供了复制原模型参数字典的方法，用于流式模型的转换。

`online\dpcrn_stream.py` 提供了流式模型的转换及推理过程，注意对流式模型，输入张量的时间维度始终为 1.

### 2. 转换为 ONNX 模型
对于流式模型，转换为 ONNX 时无需考虑时间维度的问题，但最好在 `forward` 函数中指定所有的输入张量，而不是像 `online\dpcrn_stream.py` 中那样用列表来代替。

`online\dpcrn_stream_onnx.py` 提供了流式 ONNX 模型的转换及推理过程，并评估了其在 ONNXRuntime 的推理速度。

### 3. 转换为 Engine 模型
我们同样使用 TensorRT 官方提供的转换工具 `trtexec.exe` 来进行模型转换。

`online\dpcrn_stream_trt.py` 提供了流式 Engine 模型的导出及推理，并评估了 Engine 模型的推理速度，结果如下表。

| **模型格式** | **推理框架** | **推理平台** | **平均推理速度 （ms）** | **最大推理速度 （ms）** | **最小推理速度 （ms）** |
|:-----------:|:-----------:|:-----------:|:----------------------:|:----------------------:|:----------------------:|
| ONNX | ONNXRuntime | CPU |1.0 | 3.1 | 0.9 |
| Engine |TensorRT| CUDA |2.2 | 4.7 | 1.8 |

其中推理速度的评估重复进行了 1000 次，这里推理速度的定义为：音频处理时长 / 音频总帧数。可以看到，使用 TensorRT 推理反而比使用 ONNXRuntime 更慢，这是因为对于高吞吐的流式模型，使用 TensorRT 推理时数据从 CUDA 向 CPU 迁移将会占用一定的时间。只有当模型在 CPU 上推理速度称为瓶颈时，使用 TensorRT 在 CUDA 上推理才有正面效果。
